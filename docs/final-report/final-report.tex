\documentclass[12pt,oneside]{memoir}

\usepackage{nus-bcomp-fyp}

\addbibresource{references.bib}

\title{Benchmarking and Improving OCR Systems for Southeast Asian Languages}
\author{Qiu Jiasheng, Jason}
\department{Department of Computer Science}
\faculty{School of Computing}
\university{National University of Singapore}
\academicyear{2024/2025}
\projectid{H0792230}
\supervisor{A/P Min-Yen Kan}
\advisor{Tongyao Zhu}

\begin{document}
\frontmatter

\pagestyle{plain}

\makecover

\setcounter{page}{1}

\maketitle
\addcontentsline{toc}{chapter}{Title}

\chapter*{\centering\large Abstract}
\addcontentsline{toc}{chapter}{Abstract}

While Optical Character Recognition (OCR) has been widely studied for high-resource languages such as English and Chinese, the efficacy and limitations of OCR models on Southeast Asian (SEA) languages remain largely unexplored. This study aims to bridge this gap by evaluating OCR technologies for SEA languages and exploring script-specific challenges. We propose a pipeline to collect textual data from Wikipedia and benchmark open-source OCR tools. Additionally, we demonstrate the potential of fine-tuning existing models on SEA languages, aiming to expand OCR capabilities for these languages.

\vspace{20pt}
Subject Descriptors:

\hspace*{0.3in} H.3.3 Information Search and Retrieval

\hspace*{0.3in} I.2.7 Natural Language Processing

\hspace*{0.3in} I.2.10 Vision and Scene Understanding

Keywords:

\hspace*{0.3in} Optical Character Recognition, Southeast Asian Languages

Implementation Software and Hardware:

\hspace*{0.3in} Python, Tesseract, EasyOCR

\chapter*{\centering\large Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}

I would like to thank my supervisor, A/P Kan Min-Yen, and my advisor, Tongyao Zhu, for their invaluable guidance and mentorship. Their encouragement and constructive guidance have been a significant source of inspiration throughout the project.

\listoffigures
\listoftables
\tableofcontents

\mainmatter

\chapter{Introduction}
Current research in Natural Language Processing (NLP) is heavily concentrated on only 20 of the 7,000 languages in the world \parencite{magueresse-etal-2020}.
In particular, Southeast Asia (SEA) is home to over 1,000 languages but remains a relatively under-researched region in NLP \parencite{aji-etal-2023}.
A similar trend can be observed in Optical Character Recognition (OCR) research, where the focus is predominantly on high-resource languages \parencite{salehudin-etal-2023, smith-2007}, leaving many SEA languages underserved.

OCR, the process of converting textual images into machine-readable formats, offers significant potential for languages with limited datasets. While many scanned documents and books in these low-resource languages are available online, the text within them often remains inaccessible due to formats like images and PDFs. By extracting the text from these documents, OCR can generate valuable datasets for low-resource languages, which can then be used for downstream NLP tasks, such as machine translation and named-entity recognition \parencite{agarwal-and-anastasopoulos-2024, ignat-etal-2022}.
Therefore, studying OCR performance on SEA languages is crucial to accelerating NLP research and technology development in the region.

While OCR has been widely studied for high-resource languages such as English and Chinese, the efficacy and limitations of OCR models on SEA languages remain largely unexplored.
To address this gap, this study presents a pipeline to collect textual data from Wikipedia and benchmark several open-source OCR tools on the collected data. Additionally, we explore the potential of fine-tuning existing models to improve OCR performance on SEA languages. The primary objective is to evaluate and enhance the performance of OCR technologies on SEA languages, thereby advancing NLP applications in this linguistically diverse region.

Specifically, this project seeks to answer the following research questions (RQs):

\begin{itemize}
    \item \textbf{RQ1:} How do popular OCR tools perform on SEA scripts?
    \item \textbf{RQ2:} What script-related challenges affect OCR accuracy on SEA languages?
    \item \textbf{RQ3:} What fine-tuning techniques can enhance OCR accuracy on SEA languages?
\end{itemize}

\chapter{Related Work}

\section{Overview of OCR Systems}

Most OCR systems consist of two stages: Text detection and text transcription.
Text detection identifies text present in an image and extracts cropped regions containing the detected text. 
A text transcription model then converts these cropped images into text.
Generally, separate models are used for each stage, allowing for greater training flexibility and a clearer understanding of challenges within each component \parencite{subramani-etal-2023}. 
More recently, end-to-end models that combine both stages have shown promise in reducing errors for certain use cases \parencite{feng-etal-2019}.

\subsection{Evolution of OCR Models}

Early OCR models employ traditional machine learning techniques, such as K-nearest Neighbors (KNN) and Support Vector Machines (SVMs), to classify textual characters from cropped images.
Tesseract, an established OCR engine developed since the 1990s, recognizes character patterns by extracting small fragments of character outlines as features \parencite{smith-2013}.
These features are then classified into character clusters using an optimized KNN algorithm.
While effective for structured text, these traditional approaches struggled with variations in handwriting, fonts, and image distortions \parencite{subramani-etal-2023}.

The rise of deep learning brought significant advancements in OCR.
Convolutional Neural Networks (CNNs) improve feature extraction by automatically detecting edges, textures, and shapes within text images.
Unlike traditional handcrafted features, CNNs learn visual patterns by applying small filters across an image. 
The Character Region Awareness for Text Detection (CRAFT) algorithm, for example, uses a fully convolutional network to achieve state-of-the-art character localization \parencite{baek-etal-2019}.
For text transcription, Recurrent Neural Networks (RNNs) have been widely adopted due to their ability to model sequential dependencies over time.
Tesseract v4 integrated a Long Short-Term Memory (LSTM) model, a specialized type of RNN, to recognize entire lines of text instead of individual characters \parencite{tesseract-2025}.
By combining CNNs for feature extraction and RNNs for sequence modeling, \textcite{shi-etal-2015} proposed the Convolutional Recurrent Neural Network (CRNN), which significantly improved text recognition accuracy in end-to-end OCR systems.

More recently, transformer-based models have emerged as a powerful alternative.
Unlike CNNs and RNNs, transformers process entire input sequences in parallel using self-attention mechanisms, which allows them to capture long-range dependencies in text images more efficiently \parencite{vaswani-2017}.
This approach avoids image-specific inductive biases present in CNNs, such as the assumption that neighboring pixels are relevant.
TrOCR, an end-to-end model that combines an image transformer and a separate text transformer, demonstrates another advantage of transformers: the ability to leverage self-supervised pre-training \parencite{li-etal-2021}. 
Since transformers can be pre-trained individually to learn useful patterns from unlabeled images and text, there is less reliance on manually annotated OCR training data to achieve high accuracy.
Going beyond traditional text recognition, General OCR Theory (GOT) is another transformer-based model that extends character recognition capabilities to non-text elements, such as sheet music, charts, and geometric shapes \parencite{wei-etal-2024}.
By integrating Large Visual-Language Models (LVLMs), GOT seeks to address the bottlenecks of traditional OCR systems, which often struggle with generalization.
As transformer-based OCR continues to evolve, these models are expected to push the boundaries of text recognition, enabling more flexible and adaptable OCR systems for diverse applications.

\section{Benchmarking OCR on Low-resource Languages}

To evaluate OCR performance accurately, textual data in the form of images or PDFs paired with reliable ground truth is essential. 
Similar to most NLP tasks, data scarcity poses a major obstacle to advancing OCR technology in low-resource languages. The limited availability of annotated textual data restricts both model training and evaluation, leading to disparities in OCR accuracy across different scripts.
OCR tools generally perform better on Latin-based scripts \parencite{hegghammer-2022, ignat-etal-2022}, partly due to market incentives that prioritize the development of English-language OCR systems, resulting in more extensive training data and refinement. Beyond data availability, the complexity of scripts with ornate diacritics or unique letter shapes often yield lower OCR accuracy \parencite{agarwal-and-anastasopoulos-2024}.

A recent study by \textcite{ignat-etal-2022} provides the most relevant benchmarking of OCR on SEA languages.
Their benchmark grouped 60 low-resource languages by region and script, including SEA languages such as Khmer, Lao, Burmese, Thai, and Vietnamese.
They found that while OCR models perform well on synthetic SEA-language data, their accuracy drops significantly on real-world data.
This discrepancy underscores the need for more diverse and realistic training datasets to improve OCR outcomes for SEA languages.

\section{Using Synthetic Data for OCR Evaluation}

To bridge the gap in data availability, many studies rely on artificial images and PDFs generated from plain text to create evaluation datasets.
For instance, \textcite{ignat-etal-2022} generated synthetic PDFs from the Flores 101 dataset, which consists of text from Wikipedia in 101 languages.
Expanding on this approach, \textcite{gupte-etal-2021} developed an open-source Python package that creates document images from plain text, incorporating several document styling templates.
These methods enable the large-scale generation of high-quality, low-resource language data with corresponding ground truth annotations.

However, one challenge with artificial datasets is their tendency to lack the imperfections found in real-world documents. 
Real-world scanned documents often feature complex layouts, stains, and handwritten scribbles \parencite{hegghammer-2022}. 
Studies have shown that OCR systems often perform better on synthetic datasets than on real-world data, highlighting a gap in generalization \parencite{ignat-etal-2022}.
To address this, researchers frequently apply noise augmentation to synthetic documents. 
Common techniques include changing the font style, size, color, and letter spacing, as well as adding Gaussian blur, bleed-through effects, and salt-and-pepper noise \parencite{gupte-etal-2021, ignat-etal-2022}.
These modifications help artificial datasets better approximate the challenges of real-world OCR tasks.

\section{Fine-tuning OCR Systems}

To enhance OCR performance in new domains with limited labeled data, many studies explore fine-tuning, or further training pre-trained models on a smaller, task-specific dataset. 
Instead of training from scratch, fine-tuning updates a model's existing weights, allowing it to adapt to new datasets while retaining prior knowledge. 
For instance, \textcite{parres-and-paredes-2023} demonstrated that transformer-based models can successfully adapt to new languages and historical documents with minimal training data, achieving competitive OCR performance. 
Similarly, \textcite{laurent-and-lauar-2024} fine-tuned the English TrOCR model for Spanish text, yielding strong results.
Fine-tuning thus provides an effective strategy for overcoming the scarcity of labeled data in new OCR domains, particularly for low-resource languages, while maintaining high accuracy.

% TODO: Add techniques for fine-tuning?
% TODO: Add how my research addresses previous works' limitations

\chapter{Methodology}

To answer the research questions, this study conducted the following three experiments to benchmark and improve OCR performance on SEA languages:

\begin{itemize}
    \item \textbf{Experiment 1:} Benchmarking on Real-world Data
    \item \textbf{Experiment 2:} Benchmarking on Synthetic Data
    \item \textbf{Experiment 3:} Fine-tuning for Vietnamese and Thai
\end{itemize}

\section{Experiment Setup}

\subsection{Languages}

In this study, we chose to benchmark on English, Indonesian, Vietnamese, and Thai. English serves as a baseline comparison due to its extensive OCR research and established tool support. Meanwhile, Indonesian, Vietnamese, and Thai were selected as a representative subset of SEA languages for several reasons.

Firstly, these three languages encompass a range of script types: Latin scripts for Indonesian, Latin scripts with diacritics for Vietnamese, and Brahmic scripts for Thai. 
By covering these scripts, we capture a broad spectrum of orthographic features, from diacritics to tone marks and from Latin-based scripts to complex character shapes. This allows us to examine how these unique linguistic features impact OCR performance. 
Furthermore, many other SEA languages, including Malay, Filipino, and Cebuano, use modified Latin scripts, while languages like Khmer, Burmese, and Javanese use Brahmic scripts. Thus, findings from this study can be applied to other languages with similar script types, accelerating OCR research in the region.

\begin{table}[ht]
    \centering
    \caption{Benchmarked Languages}
    \label{table:languages}
    \begin{tabular}{llll}
        \toprule
        & Speaker Population & Script Type & Example\\ 
        \midrule
        English & 1.5 billion & Latin & Good morning\\
        Indonesian & 252 million & Latin & Selamat pagi\\
        Vietnamese & 97 million & Latin with diacritics & Chào buổi sáng\\
        Thai & 71 million & Brahmic & {\fontspec{Tahoma} สวัสดีตอนเช้า}\\
        \bottomrule
        \multicolumn{4}{c}{\footnotesize Note: Speaker population data from Wikipedia (\citeyear{list-of-languages-by-total-number-of-speakers-2025}).}
    \end{tabular}
\end{table}

Secondly, the wide usage of these languages makes it feasible to obtain textual data. The high number of speakers, active online communities, and abundant digital content ensure sufficient resources for OCR benchmarking. Their prominence in SEA further highlights their relevance, as improving OCR for these languages benefits a large portion of the region's population.

While this study covers only a small fraction of the languages spoken in SEA, the selection of these languages provides a strong starting point, as they cover popular script types and offer abundant online data for benchmarking.

\subsection{Data Source}

To collect textual data, this study uses Wikipedia due to its accessibility and multilingual scope.
Wikipedia articles can be converted into images via screenshots, simulating real-world OCR scenarios. 
The platform also offers a convenient Application Programming Interface (API) that allows retrieval of plain text from most articles, serving as a reliable reference for evaluating OCR accuracy and generating synthetic documents.
Moreover, the availability of large corpora in various SEA languages, including Thai, Vietnamese, Indonesian, Tamil, and Burmese, makes Wikipedia suitable for this study's language needs \parencite{list-of-wikipedias-2024}.

\subsection{OCR Systems}

In our selection of OCR systems for benchmarking, we prioritize open-source solutions that support a diverse range of SEA languages, promoting accessibility and reusability for the proposed evaluation pipeline. 
Additionally, we aim to include models with different underlying architectures, enabling a more comprehensive assessment of their performance across different languages.
Consequently, we selected EasyOCR, Tesseract, and General OCR Theory (GOT), each open-source and representing distinct modeling approaches to OCR.

\begin{table}[ht]
    \centering
    \caption{Benchmarked OCR Systems}
    \label{table:ocr-systems}
    \begin{tabular}{llll}
        \toprule
        & Architecture & \# Supported Languages\\ 
        \midrule
        EasyOCR & CRAFT + CRNN & 83 (includes all benchmarked languages) & \\
        Tesseract & LSTM & 116 (includes all benchmarked languages)\\
        GOT & VED & 2 (English and Simplified Chinese)\\
        \bottomrule
    \end{tabular}
\end{table}

EasyOCR is a modern OCR framework that integrates a text detection model based on the Character Region Awareness for Text Detection (CRAFT) algorithm with a recognition model utilizing a Convolutional Recurrent Neural Network (CRNN) \parencite{easy-ocr-2025}.
Readily available as a Python package, EasyOCR supports 83 languages, including English, Indonesian, Vietnamese, and Thai.

Tesseract is one of the most well-known open-source OCR engines. Since releasing version 4 in 2018, Tesseract uses an underlying Long Short-Term Memory (LSTM) model for line recognition \parencite{tesseract-2025}.
Similar to EasyOCR, Tesseract is accessible via a Python package and supports the four chosen languages in this study 

GOT is a transformer-based model designed to recognize artificial characters beyond traditional text, such as sheet music, mathematical equations, and charts \parencite{wei-etal-2024}. 
Using a Vision Encoder Decoder (VED) architecture with 580 million parameters, GOT fine-tunes ViTDeT\footnote{ViTDeT is an object detection model using the Vision Transformer (ViT) as a backbone network \parencite{li-etal-2022}.} as its vision encoder and Qwen-0.5B\footnote{Qwen-0.5B is a Large Language Model (LLM) with 500 million parameters developed by Alibaba Cloud \parencite{qwen-2025}.} as its language decoder. 
GOT is conveniently available on Hugging Face\footnote{\url{https://huggingface.co/stepfun-ai/GOT-OCR2_0}}.
While GOT officially supports only English and Simplified Chinese, it does not support Indonesian, Vietnamese, or Thai. 
This study seeks to address this limitation by fine-tuning GOT on these languages in Section \ref{section:experiment-3}.

\subsection{Evaluation Metrics}

\begin{equation}
    CER = \frac{I + D + S}{N}
    \label{equation:cer}
\end{equation}

Similar to most similar studies, we utilize Character Error Rate (CER) and Word Error Rate (WER) as our evaluation metrics to measure OCR accuracy \parencite{hegghammer-2022, ignat-etal-2022}. 
CER measures the accuracy of character recognition and is calculated using the Levenshtein or edit distance, which represents the minimum number of single-character insertions (I), deletions (D), and substitutions (S) required to transform one word into another. 
As shown in Equation \ref{equation:cer}, CER is defined as the edit distance between the OCR-predicted text and ground truth text, divided by the total number of characters in the ground truth text (N). 
A lower CER value indicates higher accuracy, with 0 representing perfect recognition. 
Notably, CER can exceed 1 when there is a significant number of insertions. 
WER serves as the word-based counterpart to CER.

\section{Experiment 1: Benchmarking on Real-world Data} \label{section:experiment-1}

To explore the performance of OCR tools on SEA scripts (RQ1), Experiment 1 benchmarks OCR systems using screen-captured, real-world data from Wikipedia.
Unlike synthetic data, these screenshots contain formatting variations and complex layouts that better reflect real-world OCR challenges. 
This approach ensures that the evaluation closely mirrors practical use cases, where OCR tools must handle noisy and visually complex text.

\subsection{Data Collection}

To ensure substantial data availability across our chosen languages, we compiled a dataset of 100 popular Wikipedia articles. Specifically, we selected the 20 most viewed English articles from each of five categories: people, present countries, cities, life, and buildings and structures \parencite{wikipedia-popular-pages-2024}.
These categories were also chosen to create a diverse corpus in terms of content. 
Table \ref{table:real-world-wikipedia-dataset} lists the articles included in our dataset.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{images/data-collection.png}
    \caption{Pipeline for data collection from Wikipedia}
    \label{figure:data-collection}
\end{figure}

From the dataset of 100 Wikipedia articles, we collected article images and ground 
truth article text in our selected languages using Python, 
Selenium\footnote{\href{https://selenium-python.readthedocs.io}{Selenium} is a 
framework for automating web browsers, commonly used for web scraping by programmatically 
interacting with websites.}, and the MediaWiki Action API\footnote{The \href{https://www.mediawiki.org/wiki/API:Main_page}{MediaWiki Action API} allows 
access to wiki page operation features such as search and retrieval.}. Figure \ref{figure:data-collection} illustrates the overall pipeline 
for data collection. The detailed steps are as follows:

\begin{enumerate}
    \item Manually compile the dataset’s article names and URLs in English.
    \item Fetch the article names and URLs in Thai, Vietnamese, and Indonesian from the MediaWiki Action API.
    \item Download the article PDFs in all languages using Selenium.
    \item Convert the article PDFs into PNG images, with each image representing one page of the PDF.
    \item Download the ground truth article text into TXT files from the MediaWiki Action API.
\end{enumerate}

The end result is a real-world Wikipedia dataset with 3,590 English images, 1,450 Indonesian images, 1,925 Vietnamese images, and 1,011 Thai images.
Figure \ref{figure:real-world-data} presents sample collected images.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.65\textwidth]{images/real-world-data.png}
    \caption{Sample English and Thai images collected from Wikipedia}
    \label{figure:real-world-data}
\end{figure}

\subsection{OCR Evaluation} \label{section:ocr-evaluation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{images/ocr-evaluation.png}
    \caption{Pipeline for OCR evaluation}
    \label{figure:ocr-evaluation}
\end{figure}

After collecting the images and corresponding ground truth text, we ran the OCR tools and evaluated the CER and WER for each article.
Figure \ref{figure:ocr-evaluation} summarizes the overall pipeline for OCR evaluation. The detailed steps are as follows:

\begin{enumerate}
    \item \textbf{Apply OCR:} Apply the OCR tools on the article images.
\end{enumerate}

To run EasyOCR, Tesseract, and GOT on all 7,976 images, we used Slurm for job scheduling and execution on the SoC Compute Cluster at the School of Computing, National University of Singapore. Python and shell scripts were utilized to automate OCR execution.

\begin{enumerate}
    \setcounter{enumi}{1}
    \item \textbf{Data Cleaning:} Perform data cleaning on the predicted article text and ground truth article text.
\end{enumerate}

The raw predicted text generated by the OCR tools exhibited some consistent formatting issues.
For instance, Tesseract adds an additional space character after every predicted character.
The article images also included references and in-text citations, which are not present in the ground truth.
To address these issues, we performed data cleaning to align the output text more closely with the ground truth.

\begin{enumerate}
    \setcounter{enumi}{2}
    \item \textbf{Evaluation:} Compute the CER and WER between the predicted text and the ground truth text using JiWER\footnote{\href{https://pypi.org/project/jiwer/}{JiWER} is a Python package designed for fast calculation of CER and WER.}.
    \item \textbf{Data Validation:} Manually review articles with outlier CER values for incorrect ground truth text.
\end{enumerate}

To prevent erroneous results, we implemented a data validation step that automatically checked the CERs for each article.
Articles were flagged as outliers if their CER exceeded two standard deviations from the mean \parencite{cousineau-and-chartier-2010}. 
We manually reviewed these outlier articles for anomalies, resulting in the removal 
of seven articles where the images and ground truth texts contained different content.

% TODO: update anomaly review

\section{Experiment 2: Benchmarking on Synthetic Data}

Unlike Experiment 1, Experiment 2 generates images from plain text for benchmarking. 
This approach allows us to introduce controlled distortions to the dataset, enabling an analysis of OCR robustness against noise on SEA languages and offering a different perspective on RQ1.
Additionally, using synthetic data minimizes annotation errors, allowing us to better isolate script-related errors (RQ2).

\subsection{Synthetic Data Generation}

Using the article text collected from Experiment 1, we generated synthetic article images with Python and WeasyPrint\footnote{\url{https://pypi.org/project/weasyprint/}}, a Python package for converting HTML pages into PDF documents.
To introduce font noise into the dataset, we randomly applied HTML tags to a specified ratio of space-separated words.
For instance, a \texttt{<b>} tag could be added randomly to make words appear in bold.
The following steps summarizes the process of data generation:

\begin{enumerate}
    \item If noise is needed, randomly apply HTML tags to a specified ratio of space-separated words.
    \item Generate synthetic article PDFs using the ground truth text collected from Experiment 1 with WeasyPrint.
    \item Convert the article PDFs into PNG images, with each image representing one page of the PDF.
\end{enumerate}

\subsection{OCR Evaluation on Different Types of Noise}

To investigate how different types of noise impact OCR performance (RQ1), we generated separate datasets, each containing a specific type of noise: bold, italic, link, or heading. 
The noise was randomly applied to a specified percentage of words in each dataset. 
Additionally, a control dataset without noise was created for comparison.

\begin{table}[ht]
    \centering
    \caption{Types of noise applied}
    \label{table:noises}
    \begin{tabular}{lcc}
        \toprule
        & HTML Tag & Ratio\\
        \midrule
        Bold & \texttt{<b>} & 0.3\\
        Italic & \texttt{<i>} & 0.3\\
        Link & \texttt{<a>} & 0.3\\
        Heading & \texttt{<h1>} & 0.03\\
        No noise & - & -\\
        \bottomrule
    \end{tabular}
\end{table}

Table \ref{table:noises} summarizes the noise types, their corresponding HTML tags, and the percentage of words affected.
A smaller ratio was applied for headings because headings typically appear less frequently in natural text compared to other formatting elements like bold or italics.
Limiting the number of headings also helped reduce the number of pages generated, ensuring a more manageable dataset size.
Figure \ref{figure:noises} presents a sample synthetic image with different types of noise.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{images/noises.png}
    \caption{Sample synthetic image with different types of noise}
    \label{figure:noises}
\end{figure}

The selected noise types introduce formatting-based distortions commonly found in digital text and web-based content, posing unique challenges for OCR systems.
Bold and italic formatting, frequently used for emphasis in scanned documents and articles, may alter character shapes and affect recognition accuracy.
Links introduce underlining and color changes, which can interfere with OCR systems.
Headings, often bold and larger in size, may also impact recognition.
Evaluating these effects thus help assess OCR robustness in processing real-world digital text.

After generating the five separate datasets, we ran the OCR tools and evaluated their performance on each dataset, following the approach described in Section \ref{section:ocr-evaluation}.

\subsection{Error Classification by Character Type}

Another area of interest in Experiment 2 was the effect of unique script characteristics, such as diacritics and punctuation, on OCR accuracy (RQ2).
Using OCR results from the control dataset without noise, we categorized misclassifications by 11 character types commonly found in English, Indonesian, Vietnamese, and Thai.

\begin{table}[ht]
    \centering
    \caption{Character types used for error classification}
    \label{table:character-types}
    \begin{tabular}{ll}
        \toprule
        & Included Characters\\
        \midrule
        Arabic digit & 0-9\\
        Thai digit & {\fontspec{Tahoma} ๐-๙}\\
        Latin letter & a-zA-Z\\
        Latin letter with diacritic& à-ỹ\\
        Thai letter& {\fontspec{Tahoma} ก-ฮ}\\
        Thai diacritic& {\fontspec{Tahoma}  ้ ๊๋ ็ ์}\\
        Punctuation& .,!?;:()-"'\\
        Thai punctuation& {\fontspec{Tahoma} ๆฯ}\\
        Vietnamese punctuation& «»\\
        Whitespace& \textvisiblespace\\
        Other& \\
        \bottomrule
    \end{tabular}
\end{table}

We used the Levenshtein\footnote{\url{https://pypi.org/project/Levenshtein/}} Python package to identify edit operations (insertions, deletion, and substitutions) and classified misrecognized characters using RegEx\footnote{\url{https://docs.python.org/3/library/re.html}}. 
Table \ref{table:character-types} lists the character types we analyzed, with all uncategorized characters grouped under "Other".

\section{Experiment 3: Fine-tuning for Vietnamese and Thai} \label{section:experiment-3}

To explore how fine-tuning can enhance OCR accuracy on SEA languages (RQ3), Experiment 3 fine-tunes GOT for Vietnamese and Thai and compares the fine-tuned model with EasyOCR and Tesseract. 
We chose to fine-tune for Vietnamese and Thai because GOT does not natively support these languages. 
Although GOT does not support Indonesian, we did not fine-tune for it, since the model already achieves state-of-the-art results for Indonesian, as demonstrated in Experiment 1 (See Section ). 

\subsection{Fine-tuning GOT}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{images/fine-tuning.png}
    \caption{Pipeline for fine-tuning GOT}
    \label{figure:fine-tuning}
\end{figure}

Fine-tuning our model effectively required a large volume of high-quality data. 
To ensure the reliability of our training data, we generated additional synthetic data without noise. 
After collecting more plain text from Wikipedia, we created 960 images for training and 50 images for testing, for both Vietnamese and Thai.
To investigate how much training data is needed for effective fine-tuning, we randomly selected samples from the training dataset to create datasets with 50, 100, 200, 400, and 960 images. 
These sample sizes were chosen to follow an increasing scale, approximating diminishing returns as data size increases.
For each dataset, the same base model (GOT) was fine-tuned for three epochs, resulting in different fine-tuned models based on the varying dataset sizes.

For fine-tuning, we used SWIFT\footnote{\url{https://github.com/modelscope/ms-swift}}, a user-friendly framework that supports fine-tuning GOT\footnote{\url{https://github.com/modelscope/ms-swift/issues/2122}}. 
Using a single NVIDIA Titan V GPU on the SoC Compute Cluster, we fine-tuned our model using Low-Rank Adaptation (LoRA), which optimizes only small adapter layers while keeping the pre-trained model weights frozen \parencite{hu-etal-2021}.
This approach allows for efficient fine-tuning with reduced memory and computational costs.

\subsection{OCR Evaluation on Fine-tuned GOT}

We ran each fine-tuned GOT model on the test dataset and evaluated their performance, following the approach described in section \ref{section:ocr-evaluation}.
We also ran EasyOCR and Tesseract on the test dataset for comparison.

\chapter{Results and Analysis}

In this chapter, we analyze and evaluate the results of the experiments to provide answers to the research questions.

\section{RQ1: How do popular OCR tools perform on SEA scripts?}

\subsection{OCR Accuracy on Real-world Data}

\begin{table}[ht]
    \centering
    \caption{Average CER and WER on real-world data}
    \label{table:ocr-accuracy-on-real-world-data}
    \begin{tabular}{lcccccc}
        \toprule
        & \multicolumn{3}{c}{\textbf{CER}} & \multicolumn{3}{c}{\textbf{WER}}\\
        \cmidrule(lr){2-4}\cmidrule(lr){5-7}
        & EasyOCR & Tesseract & GOT & EasyOCR & Tesseract & GOT\\
        \midrule
        English & \\
        \midrule
        Indonesian & \\
        Vietnamese & \\
        Thai & \\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{OCR Accuracy on Synthetic Data}

\begin{table}[ht]
    \centering
    \caption{Average CER and WER on synthetic data}
    \label{table:ocr-accuracy-on-synthetic-data}
    \begin{tabular}{lcccccc}
        \toprule
        & \multicolumn{3}{c}{\textbf{CER}} & \multicolumn{3}{c}{\textbf{WER}}\\
        \cmidrule(lr){2-4}\cmidrule(lr){5-7}
        & EasyOCR & Tesseract & GOT & EasyOCR & Tesseract & GOT\\
        \midrule
        English & 0.03 & 0.09 & 0.02 & 0.04 & 0.01 & 0.04\\
        \midrule
        Indonesian & 0.03 & 0.09 & 0.02 & 0.06 & 0.01 & 0.05\\
        Vietnamese & 0.10 & 0.17 & 0.03 & 0.04 & - & -\\
        Thai & 0.07 & 0.68 & 0.09 & 0.64 & - & -\\
        \bottomrule
    \end{tabular}
\end{table}

\subsection{Runtime}

\begin{table}[ht]
    \caption{Average OCR runtime per page (seconds)}
    \label{table:runtime}
    \centering
    \begin{tabular}{lccc}
        \toprule
        & EasyOCR & Tesseract & GOT\\ 
        \midrule
        English & 3.23 & 11.68 & 24.35\\
        Indonesian & 2.92 & 13.19 & 31.44\\
        Vietnamese & 3.91 & 11.80 & -\\
        Thai & 2.32 & 16.76 & -\\
        \bottomrule
    \end{tabular}
\end{table}

\section{RQ2: What script-related challenges affect OCR accuracy on SEA languages?}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/error-classification-english.png}
    \caption{Error classification by character type for English articles}
    \label{figure:error-classification-english}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{images/error-classification-indonesian.png}
    \caption{Error classification by character type for Indonesian articles}
    \label{figure:error-classification-indonesian}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{images/error-classification-vietnamese.png}
    \caption{Error classification by character type for Vietnamese articles}
    \label{figure:error-classification-vietnamese}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{images/error-classification-thai.png}
    \caption{Error classification by character type for Thai articles}
    \label{figure:error-classification-thai}
\end{figure}

\section{RQ3: What fine-tuning techniques can enhance OCR accuracy on SEA languages?}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{images/fine-tuned-got-vietnamese.png}
    \caption{Performance of fine-tuned GOT on Vietnamese}
    \label{figure:fine-tuned-got-vietnamese}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{images/fine-tuned-got-thai.png}
    \caption{Performance of fine-tuned GOT on Thai}
    \label{figure:fine-tuned-got-thai}
\end{figure}

\chapter{Discussion}

\chapter{Conclusion}

\printbibliography[title={References}]

\clearpage
\appendix
\renewcommand{\chaptername}{Appendix}

\chapter{Wikipedia Dataset}

\begin{table}[ht]
    \centering
    \caption{Dataset of 100 Wikipedia articles used for benchmarking real-world data in Experiment 1 (See Section \ref{section:experiment-1})}
    \label{table:real-world-wikipedia-dataset}
    \begin{tabular}{p{1in}p{4.8in}}
        \toprule
        \textbf{Category} & \textbf{Articles} \\
        \midrule
        People & Elizabeth II, Barack Obama, Michael Jackson, Elon Musk, Lady Gaga, Adolf Hitler, Eminem, Lionel Messi, Justin Bieber, Freddie Mercury, Kim Kardashian, Johnny Depp, Steve Jobs, Dwayne Johnson, Michael Jordan, Taylor Swift, Stephen Hawking, Kanye West, Donald Trump, Cristiano Ronaldo\\
        \midrule
        Present countries & United States, India, United Kingdom, Canada, Australia, China, Russia, Japan, Germany, France, Singapore, Israel, Pakistan, Philippines, Brazil, Italy, Netherlands, New Zealand, Ukraine, Spain\\
        \midrule
        Cities$^a$ & New York City, London, Hong Kong, Los Angeles, Dubai, Washington, D.C., Paris, Chicago, Mumbai, San Francisco, Rome, Monaco, Toronto, Tokyo, Philadelphia, Machu Picchu, Jerusalem, Amsterdam, Boston, Angelsberg\\
        \midrule
        Life & Cat, Dog, Animal, Lion, Coronavirus, Tiger, Human, Dinosaur, Elephant, Virus, Horse, Photosynthesis, Evolution, Apple, Bird, Mammal, Potato, Polar bear, Shark, Snake\\
        \midrule
        Buildings and structures$^b$ & Taj Mahal, Burj Khalifa, Statue of Liberty, Great Wall of China, Eiffel Tower, Berlin Wall, Stonehenge, Mount Rushmore, Colosseum, Auschwitz concentration camp, Great Pyramid of Giza, One World Trade Center, Empire State Building, White House, Petra, Large Hadron Collider, Hagia Sophia, Golden Gate Bridge, Panama Canal, Angkor Wat\\
        \bottomrule
        \multicolumn{2}{l}{\footnotesize $^a$ Singapore was replaced because it's already listed under present countries.}\\
        \multicolumn{2}{l}{\footnotesize $^b$ Machu Picchu was replaced becuase it's already listed under cities.}
    \end{tabular}
\end{table}

\end{document}
