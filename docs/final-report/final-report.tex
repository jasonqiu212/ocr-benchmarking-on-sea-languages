\documentclass[12pt,oneside]{memoir}

\usepackage{nus-bcomp-fyp}

\addbibresource{references.bib}

\definecolor{DarkBlue}{RGB}{0, 42, 97}
\hypersetup{
    colorlinks=true,    % Enables colored links
    linkcolor=black,    % Color of internal links
    citecolor=black,    % Color of citations
    urlcolor=DarkBlue   % Color of external links
}

\title{Benchmarking and Improving OCR Systems for Southeast Asian Languages}
\author{Qiu Jiasheng, Jason}
\department{Department of Computer Science}
\faculty{School of Computing}
\university{National University of Singapore}
\academicyear{2024/2025}
\projectid{H0792230}
\supervisor{A/P Min-Yen Kan}
\advisor{Tongyao Zhu}

\begin{document}
\frontmatter

\pagestyle{plain}

\makecover

\setcounter{page}{1}

\maketitle
\addcontentsline{toc}{chapter}{Title}

\chapter*{\centering\large Abstract}
\addcontentsline{toc}{chapter}{Abstract}

While Optical Character Recognition (OCR) has been widely studied for high-resource languages such as English and Chinese, the efficacy and limitations of OCR models on Southeast Asian (SEA) languages remain largely unexplored.
This study aims to bridge this gap by assessing and improving the performance of OCR technologies on SEA languages.
To achieve this objective, we propose a reusable pipeline to gather SEA-language text from Wikipedia and benchmark popular OCR tools.

\vspace{20pt}
Subject Descriptors:

\hspace*{0.3in} H.3.3 Information Search and Retrieval

\hspace*{0.3in} I.2.7 Natural Language Processing

\hspace*{0.3in} I.2.10 Vision and Scene Understanding

Keywords:

\hspace*{0.3in} Optical Character Recognition, Southeast Asian Languages

Implementation Software and Hardware:

\hspace*{0.3in} Python, Tesseract, EasyOCR

\chapter*{\centering\large Acknowledgements}
\addcontentsline{toc}{chapter}{Acknowledgements}

I would like to thank my supervisor, A/P Kan Min-Yen, and my advisor, Tongyao Zhu, for their invaluable guidance and mentorship. Their encouragement and constructive guidance have been a significant source of inspiration throughout the project.

\listoffigures
\listoftables
\tableofcontents

\mainmatter

\chapter{Introduction}
Current research in Natural Language Processing (NLP) is heavily concentrated on 20 of the 7,000 languages in the world \parencite{magueresse-etal-2020}.
In particular, Southeast Asia (SEA) is home to over 1,000 languages but remains a relatively under-researched region in NLP \parencite{aji-etal-2023}.
Similar to most low-resource languages, a major challenge in developing NLP systems for SEA languages is the limited availability of datasets for the region’s languages.
Although many scanned documents and books in these low-resource languages are available online, the text within these files remains inaccessible due to formats like images and PDFs.

A solution to this problem is to use Optical Character Recognition (OCR) to extract the textual data.
OCR is the process of identifying and converting text in an image into a computer-friendly text format.
By extracting the text from these scanned documents, OCR can generate valuable datasets for low-resource languages.
The created datasets can then be used for downstream NLP tasks, such as machine translation, training large language models, and named-entity recognition \parencite{agarwal-and-anastasopoulos-2024, ignat-etal-2022}.
Therefore, studying OCR performance on SEA languages is crucial to accelerating NLP research in the region.

While OCR has been widely studied for high-resource languages such as English and Chinese, the efficacy and limitations of OCR models on SEA languages remain largely unexplored.
To address this gap, we propose a reusable pipeline to collect textual data in low-resource SEA languages from Wikipedia and benchmark popular open-source OCR tools on the collected data.
The primary objective is to benchmark and improve the performance of OCR technologies on SEA languages, thereby contributing to the advancement of NLP applications in this linguistically diverse region.
Specifically, this project seeks to answer the following research questions (RQs):

\begin{itemize}
    \item \textbf{RQ1.} How do popular OCR tools perform on SEA scripts?
    \item \textbf{RQ2.} What specific linguistic and script-related challenges affect OCR accuracy on SEA languages?
    \item \textbf{RQ3.} What techniques and recommendations can enhance OCR accuracy on SEA languages?
\end{itemize}

\chapter{Related Work}

\chapter{Methodology}

\section{Experiment Setup}

\subsection{OCR Systems}

In our selection of OCR systems for benchmarking, we prioritized open-source 
solutions that support a diverse range of SEA languages, as this approach 
enhances accessibility and reusability for the proposed evaluation pipeline. 
Consequently, we selected to use Tesseract and EasyOCR. 

Tesseract\footnote{\url{https://github.com/tesseract-ocr/tesseract}} is an established OCR engine, recognized as one of the top performers 
in the 1995 UNLV Test \parencite{rice-etal-1995}. It utilizes an underlying Long Short-Term Memory (LSTM) 
model. EasyOCR\footnote{\url{https://github.com/JaidedAI/EasyOCR}} is a modern OCR framework that integrates a text detection model 
based on the Character Region Awareness for Text (CRAFT) algorithm with a 
recognition model utilizing a Convolutional Recurrent Neural Network (CRNN). Both 
Tesseract and EasyOCR provide robust support for English, Indonesian, Vietnamese, 
and Thai, making them suitable candidates for our benchmarking study.

\subsection{Evaluation Metrics}

\begin{equation}
    CER = \frac{I + D + S}{N}
    \label{equation:cer}
\end{equation}

Similar to most OCR benchmark studies, we utilize Character Error Rate (CER) and 
Word Error Rate (WER) as our evaluation metrics \parencite{hegghammer-2022, ignat-etal-2022}. CER measures the accuracy of 
character recognition and is calculated using the Levenshtein or edit distance, 
which represents the minimum number of single-character insertions (I), deletions (D), 
and substitutions (S) required to transform one word into another. As shown in 
Equation \ref{equation:cer}, CER is defined as the edit distance between the 
OCR-predicted text and ground truth text, divided by the total number of characters 
in the ground truth text (N). A lower CER value indicates higher accuracy, with 
0 representing perfect recognition. Notably, CER can exceed 1, particularly when 
there are a significant number of insertions. WER serves as the word-based counterpart to CER.


\subsection{Source of Data}

We chose to use Wikipedia as our text corpus for several reasons.
Firstly, Wikipedia articles can be easily converted into images via screenshots, making them suitable for OCR applications. 
The platform also offers a convenient source of ground truth through its APIs that provide plain text for most articles.
Secondly, Wikipedia hosts a large corpus in several popular SEA languages, including Thai, Vietnamese, Indonesian, Tamil, and Burmese, supporting our language needs \parencite{list-of-wikipedias-2024}. 
Lastly, Wikipedia articles contain visual elements like images and tables that are common in modern real-world documents.

\subsection{Languages}
From the languages available on Wikipedia, we selected English, Indonesian, Vietnamese, and Thai text. 
English serves as a baseline for sanity checks and bug fixing.
The remaining SEA languages were chosen to capture diverse script characteristics. 
Indonesian represents Latin-based scripts, Vietnamese represents Latin scripts with diacritics, and Thai represents non-Latin scripts. 

\section{Experiment 1: Benchmarking on Real Data}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{images/data-collection.png}
    \caption{Pipeline for data collection from Wikipedia}
    \label{figure:data-collection}
\end{figure}

From the dataset of 100 Wikipedia articles, we collected article images and ground 
truth article text in our selected languages using Python, 
Selenium\footnote{\href{https://selenium-python.readthedocs.io}{Selenium} is a 
framework for automating web browsers, commonly used for web scraping by programmatically 
interacting with websites.}, and the MediaWiki Action API\footnote{The \href{https://www.mediawiki.org/wiki/API:Main_page}{MediaWiki Action API} allows 
access to wiki page operation features such as search and retrieval.}. Figure \ref{figure:data-collection} illustrates the overall pipeline 
for data collection. The detailed steps are as follows:

\begin{enumerate}
    \item Manually compile the dataset’s article names and URLs in English.
    \item Fetch the article names and URLs in Thai, Vietnamese, and Indonesian from the MediaWiki Action API.
    \item Download the article PDFs in all languages using Selenium.
    \item Convert the article PDFs into PNG images, where each image represents one page in the PDF.
    \item Download the ground truth article text into TXT files from the MediaWiki Action API.
\end{enumerate}

\section{Experiment 2: Benchmarking on Synthetic Data}
\section{Experiment 3: }

\chapter{Discussion}

\chapter{Conclusion}

\printbibliography[title={References}]

\clearpage
\appendix
\renewcommand{\chaptername}{Appendix}

\chapter{Wikipedia Article Dataset}

\begin{table}[ht]
    \centering
    \begin{tabular}{p{1in}p{3.8in}}
        \toprule
        \textbf{Category} & \textbf{Articles} \\
        \midrule
        People & Elizabeth II, Barack Obama, Michael Jackson, Elon Musk, Lady Gaga, Adolf Hitler, Eminem, Lionel Messi, Justin Bieber, Freddie Mercury, Kim Kardashian, Johnny Depp, Steve Jobs, Dwayne Johnson, Michael Jordan, Taylor Swift, Stephen Hawking, Kanye West, Donald Trump\\
        \midrule
        Present countries & United States, India, United Kingdom, Canada, Australia, China, Russia, Japan, Germany, France, Singapore, Israel, Pakistan, Philippines, Brazil, Italy, Netherlands, New Zealand, Ukraine, Spain\\
        \midrule
        Cities & New York City, London, Hong Kong, Los Angeles, Dubai, Washington, D.C., Paris, Chicago, Mumbai, San Francisco, Rome, Monaco, Toronto, Tokyo, Philadelphia, Machu Picchu, Jerusalem, Amsterdam, Boston\\
        \midrule
        Life & Cat, Dog, Animal, Lion, Coronavirus, Tiger, Human, Dinosaur, Elephant, Virus, Horse, Photosynthesis, Evolution, Apple, Bird, Mammal, Potato, Polar bear, Shark, Snake\\
        \midrule
        Buildings and structures & Taj Mahal, Burj Khalifa, Statue of Liberty, Great Wall of China, Eiffel Tower, Berlin Wall, Stonehenge, Mount Rushmore, Colosseum, Auschwitz concentration camp, Great Pyramid of Giza, One World Trade Center, Empire State Building, White House, Petra, Large Hadron Collider, Hagia Sophia, Golden Gate Bridge, Panama Canal, Angkor Wat\\
        \bottomrule
    \end{tabular}
    \caption{Dataset of 98 Wikipedia articles}
    \label{table:dataset}
\end{table}

\end{document}
